{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyramid NER\n",
    "\n",
    "This is an implementation of \"*Pyramid: A Layered Model for Nested Named Entity Recognition*\" by Jue Wang, Lidan Shou, Ke Chen and Gang Chen.\n",
    "\n",
    "Paper: https://www.aclweb.org/anthology/2020.acl-main.525\n",
    "\n",
    "**Note**: Although it is not mandatory, it is almost impossible to train this model in a CPU. You will need to run the following notebook in an environment with a GPU. In addition to this, use a bigger LM model depending on the size of the GPU memory (see variable `LM_NAME`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries & constants\n",
    "\n",
    "Install additional libraries:\n",
    "`pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_NAME = 'google/bert_uncased_L-4_H-512_A-8'\n",
    "LM_DIM = 512\n",
    "TOTAL_LAYERS = 8\n",
    "\n",
    "# The authors implementation is:\n",
    "# LM_NAME = 'dmis-lab/biobert-large-cased-v1.1'\n",
    "# LM_DIM = 1024\n",
    "# TOTAL_LAYERS = 16\n",
    "\n",
    "# Other Language Models:\n",
    "# 'google/bert_uncased_L-4_H-512_A-8'\n",
    "# 'bert-base-uncased'\n",
    "# 'dmis-lab/biobert-large-cased-v1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save model\n",
    "save_path = '../artifacts/%s/' % LM_NAME\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "model = BertModel.from_pretrained(LM_NAME)\n",
    "model.save_pretrained(save_path)\n",
    "\n",
    "slow_tokenizer = BertTokenizer.from_pretrained(LM_NAME)\n",
    "slow_tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General functions\n",
    "\n",
    "The input data is a file which looks like this:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"tokens\": [\"token0\", \"token1\", \"token2\"],\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"entity_type\": \"PER\", \n",
    "      \"span\": [0, 1],\n",
    "    },\n",
    "    {\n",
    "      \"entity_type\": \"ORG\", \n",
    "      \"span\": [2, 3],\n",
    "    },\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Now, it is necessary to encode the entities as outputs of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/train.genia.json'\n",
    "valid_file = '../data/valid.genia.json'\n",
    "test_file = '../data/test.genia.json'\n",
    "\n",
    "with open(train_file, 'r') as fp:\n",
    "    train_dataset = json.load(fp)\n",
    "with open(valid_file, 'r') as fp:\n",
    "    dev_dataset = json.load(fp)\n",
    "with open(test_file, 'r') as fp:\n",
    "    test_dataset = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset size: %d' % len(train_dataset))\n",
    "print('Dev dataset size: %d' % len(dev_dataset))\n",
    "print('Test dataset size: %d' % len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer_outputs(dataset, total_layers=16, entity_dict=None):\n",
    "    if entity_dict is None:\n",
    "        init_entity_dict = True\n",
    "        entity_dict = {'O': 0}\n",
    "    else:\n",
    "        init_entity_dict = False\n",
    "    \n",
    "    # Create dictionary of entities\n",
    "    for i, item in enumerate(dataset):\n",
    "        layer_seq = [0] * len(item['tokens'])\n",
    "        dataset[i]['layer_outputs'] = [layer_seq[i_layer:] for i_layer in range(total_layers)]\n",
    "        if init_entity_dict:\n",
    "            for entity in item['entities']:\n",
    "                entity_type = entity['entity_type']\n",
    "                b_entity_type = 'B-%s' % entity_type\n",
    "                i_entity_type = 'I-%s' % entity_type\n",
    "                if b_entity_type not in entity_dict:\n",
    "                    entity_dict[b_entity_type] = len(entity_dict)\n",
    "                if i_entity_type not in entity_dict:\n",
    "                    entity_dict[i_entity_type] = len(entity_dict)\n",
    "\n",
    "    # Generate outputs of each layer\n",
    "    last_layer = total_layers - 1\n",
    "    for item in dataset:\n",
    "        for entity in item['entities']:\n",
    "            span_start = entity['span'][0]\n",
    "            span_end = entity['span'][1]\n",
    "            b_type_id = entity_dict['B-%s' % entity['entity_type']]\n",
    "            i_type_id = entity_dict['I-%s' % entity['entity_type']]\n",
    "\n",
    "            length = span_end - span_start\n",
    "            if length >= total_layers:\n",
    "                item['layer_outputs'][last_layer][span_start] = b_type_id\n",
    "                item['layer_outputs'][last_layer][span_start+1:span_end+1] = [i_type_id]*(length)\n",
    "            else:\n",
    "                item['layer_outputs'][length][span_start] = b_type_id\n",
    "    \n",
    "    return dataset, entity_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, entity_dict = add_layer_outputs(train_dataset, total_layers=TOTAL_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset, _ = add_layer_outputs(dev_dataset, total_layers=TOTAL_LAYERS, entity_dict=entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset, _ = add_layer_outputs(test_dataset, total_layers=TOTAL_LAYERS, entity_dict=entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_idx = [x for x in list(entity_dict.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build datasets & vocabularies\n",
    "\n",
    "Once we have preprocessed the data, we can build the datasets (for Pytorch) and the vocabularies of words and characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embeddings & words vocabulary\n",
    "\n",
    "Download 100-dimensional GloVe word embeddings trained on 6B tokens. These embeddings contains $400K$ uncased tokens. Place the file `glove.6B.100d.txt` in the folder `./data/embeddings/`. Source: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_matrix(filepath, dimension, special_tokens):\n",
    "    id2word = []\n",
    "    word2id = {}\n",
    "    \n",
    "    # Read GloVe vectors\n",
    "    glove = {}\n",
    "    with open(filepath, 'rb') as f:\n",
    "        for line in f:\n",
    "            values = line.decode().split()\n",
    "            word = values[0]\n",
    "            \n",
    "            word2id[word] = len(id2word)\n",
    "            id2word.append(word)\n",
    "            glove[word] = values[1:]\n",
    "\n",
    "    # Build embedding matrix\n",
    "    embedding_matrix = np.zeros((len(glove) + len(special_tokens), dimension), dtype=np.float)\n",
    "    \n",
    "    for idx, word in enumerate(id2word):\n",
    "        embedding_matrix[idx] = np.asarray(glove[word], dtype=np.float)\n",
    "    \n",
    "    # Add special tokens and randomly initialize them\n",
    "    for special_token in special_tokens:\n",
    "        token_id = len(id2word)\n",
    "        word2id[special_token] = token_id\n",
    "        id2word.append(special_token)\n",
    "        embedding_matrix[token_id] = np.random.normal(size=dimension)\n",
    "    \n",
    "    return embedding_matrix, id2word, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = '../data/glove.6B.100d.txt'\n",
    "WORD_DIM = 100\n",
    "special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "embedding_matrix, id2word, word2id = load_embedding_matrix(GLOVE_FILE, WORD_DIM, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embedding of token \"the\"\n",
    "token_id = word2id['the']\n",
    "embedding_matrix[token_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build char vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_vocab(genia_data, lower_case=False, special_tokens=[]):\n",
    "    id2char = []\n",
    "    for item in genia_data:\n",
    "        if lower_case:\n",
    "            item_chars = [x.lower() for x in ''.join(item['tokens'])]\n",
    "        else:\n",
    "            item_chars = [x for x in ''.join(item['tokens'])]\n",
    "        id2char += item_chars\n",
    "    \n",
    "    # Remove duplicates and generate inverse dictionary\n",
    "    id2char = list(set(id2char))\n",
    "    char2id = {x:i for i, x in enumerate(id2char)}\n",
    "    \n",
    "    # Add special tokens\n",
    "    for special_token in special_tokens:\n",
    "        token_id = len(id2char)\n",
    "        char2id[special_token] = token_id\n",
    "        id2char.append(special_token)\n",
    "    \n",
    "    return id2char, char2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2char, char2id = build_char_vocab(train_dataset, special_tokens=special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers/encoders\n",
    "\n",
    "We need three different encoders for the inputs: words, chars and Language Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-level & Char-level tokenizers\n",
    "\n",
    "I use the Spacy tokenizer, which it gives the same tokens as GloVe. This library not only tokenizes texts, but also provides additional information, such as the span position of each token. <u>Note that LM (e.g. BERT) will use a different tokenizer!</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(artifacts_path='../artifacts/', lm_name='dmis-lab/biobert-large-cased-v1.1'):\n",
    "    save_path = '%s%s/' % (artifacts_path, lm_name)\n",
    "    tokenizer = BertWordPieceTokenizer('%svocab.txt' % save_path, lowercase=True)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, text, lower=True):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    \n",
    "    encoded = tokenizer.encode(text)\n",
    "    \n",
    "    tokens = encoded.tokens[1:-1]\n",
    "    spans = encoded.offsets[1:-1]\n",
    "    \n",
    "    spans = [[x[0], x[1]-1] for x in spans]\n",
    "    \n",
    "    i = len(tokens)\n",
    "    while i >= 0:\n",
    "        i -= 1\n",
    "        if re.search(r\"^##.+\", tokens[i]):\n",
    "            token = tokens[i][2:]\n",
    "            tokens[i-1] += token\n",
    "            spans[i-1][1] += len(token)\n",
    "            del tokens[i]\n",
    "            del spans[i]\n",
    "\n",
    "    return tokens, spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordInput:\n",
    "    def __init__(self, word2id, uncased=False):\n",
    "        self.uncased = uncased\n",
    "        self.word2id = word2id\n",
    "        \n",
    "    def encode(self, tok_text, padding_length=512, unk='[UNK]', pad='[PAD]'):\n",
    "        if self.uncased:\n",
    "            tok_text = [w.lower() for w in tok_text]\n",
    "\n",
    "        input_ids = [self.word2id[word] if word in self.word2id else self.word2id[unk] for word in tok_text]\n",
    "\n",
    "        # Pad if necessary\n",
    "        add_padding = padding_length - len(input_ids)\n",
    "        if add_padding > 0:\n",
    "            pad_id = self.word2id[pad]\n",
    "            input_ids = input_ids + ([pad_id] * add_padding)\n",
    "        elif add_padding < 0:\n",
    "            raise Exception('(Words) Text too long (%d / %d):' % (len(input_ids), padding_length), tok_text)\n",
    "\n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharInput:\n",
    "    def __init__(self, char2id, uncased=False):\n",
    "        self.uncased = uncased\n",
    "        self.char2id = char2id\n",
    "\n",
    "    def encode(self, tok_text, padding_length=512, char_padding=60, unk='[UNK]', pad='[PAD]'):\n",
    "        if self.uncased:\n",
    "            tok_text = [w.lower() for w in tok_text]\n",
    "\n",
    "        input_ids = []\n",
    "        \n",
    "        for token in tok_text:\n",
    "            char_ids = [self.char2id[char] if char in self.char2id else self.char2id[unk] for char in token]\n",
    "\n",
    "            # Pad char list if necessary\n",
    "            add_padding = char_padding - len(char_ids)\n",
    "            if add_padding > 0:\n",
    "                pad_id = self.char2id[pad]\n",
    "                char_ids = char_ids + ([pad_id] * add_padding)\n",
    "            elif add_padding < 0:\n",
    "                raise Exception('(Chars-1) Text too long (%d / %d):' % (len(char_ids), char_padding), text)\n",
    "\n",
    "            input_ids.append(char_ids)\n",
    "\n",
    "        # Pad token list if necessary\n",
    "        add_padding = padding_length - len(input_ids)\n",
    "        if add_padding > 0:\n",
    "            pad_id = self.char2id[pad]\n",
    "            input_ids = input_ids + ([[pad_id] * char_padding] * add_padding)\n",
    "        elif add_padding < 0:\n",
    "            raise Exception('(Chars-2) Text too long (%d / %d):' % (len(input_ids), padding_length), tok_text)\n",
    "        \n",
    "        return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = get_tokenizer(lm_name=LM_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of word-level encoding\n",
    "word_input = WordInput(word2id)\n",
    "\n",
    "text = 'This is Dimas\\' car, it\\'s blue.'\n",
    "text, _ = tokenize_text(tokenizer, text)\n",
    "word_input.encode(text, padding_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of char-level encoding\n",
    "char_input = CharInput(char2id)\n",
    "\n",
    "text = 'This is Dimas\\' car, it\\'s blue.'\n",
    "text, _ = tokenize_text(tokenizer, text)\n",
    "char_input.encode(text, padding_length=12, char_padding=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT Tokenizer\n",
    "\n",
    "BERT expects 3 different inputs:\n",
    "- Token IDs: the tokens transformed into numbers.\n",
    "- Attention Mask: sequence of `0` (if there are PAD tokens in that position) and `1` (otherwise).\n",
    "- Segments or Type IDs: sequence of `0` and `1` to distinguish between the first and the second sentence in NSP tasks. In this notebook, we do not need this input, so it will be always `0`.\n",
    "\n",
    "For example:\n",
    "```\n",
    "Text:       Is this jacksonville? Yes, it is.\n",
    "---------------------------------------------------------------------------------\n",
    "Tokens:     [CLS] Is    this  ja    ##cks ##on  ##ville ?   [SEP] Yes   ,    it   is   .    [SEP] [PAD] [PAD] ...\n",
    "Token IDs:  101   12034 10531 10201 18676 10263 12043   136 102   2160  117  1122 1110 119  102   100   100   ...\n",
    "Mask:       1     1     1     1     1     1     1       1   1     1     1    1    1    1    1     0     0     ...\n",
    "Type IDs:   0     0     0     0     0     0     0       0   0     1     1    1    1    1    1     0     0     ...\n",
    "```\n",
    "\n",
    "Note: previous token IDs are just an example, so real token IDs might be different.\n",
    "\n",
    "For further details, read BERT paper: https://arxiv.org/pdf/1810.04805.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertInput:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode(self, tok_text, padding_length=512):\n",
    "        tok_text = ['[CLS]'] + tok_text + ['[SEP]']\n",
    "        \n",
    "        # Encode context (token IDs, mask and token types)\n",
    "        token_spans = [1]\n",
    "        input_ids = []\n",
    "        type_ids = []\n",
    "        attention_mask = []\n",
    "        for token in tok_text:\n",
    "            encoded_text = self.tokenizer.encode(token)\n",
    "\n",
    "            # Create inputs\n",
    "            span = len(encoded_text.ids[1:-1])\n",
    "            token_spans += [span] + [0] * (span - 1)\n",
    "            input_ids += encoded_text.ids[1:-1]\n",
    "            type_ids += encoded_text.type_ids[1:-1]\n",
    "            attention_mask += encoded_text.attention_mask[1:-1]\n",
    "        \n",
    "        if len(input_ids) > padding_length:\n",
    "            raise Exception('(BERT) Text too long (%d / %d): %s' % (len(input_ids), padding_length, text))\n",
    "\n",
    "        # Pad if necessary. Note that \"100\" is the ID of the token \"[PAD]\" in BERT.\n",
    "        add_padding = padding_length - len(input_ids)\n",
    "        if add_padding > 0:\n",
    "            input_ids = input_ids + ([100] * add_padding)\n",
    "            attention_mask = attention_mask + ([0] * add_padding)\n",
    "            type_ids = type_ids + ([0] * add_padding)\n",
    "        \n",
    "        token_spans += ([1] * (padding_length - len(token_spans)))\n",
    "\n",
    "        # BERT inputs must be as follows: input_ids, attention_mask, token_type_ids\n",
    "        return [input_ids, attention_mask, type_ids], token_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_input = BertInput(tokenizer)\n",
    "\n",
    "text = 'Hellowing'\n",
    "text, _ = tokenize_text(tokenizer, text)\n",
    "bert_input.encode(text, padding_length=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "\n",
    "Pytorch has its own dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestedNamedEntitiesDataset(Dataset):    \n",
    "    def __init__(self, data, word_input, char_input, lm_input, padding_length=512,\n",
    "                 total_layers=16, skip_exceptions=True, max_items=-1, has_outputs=True):\n",
    "        self.total_layers = total_layers\n",
    "        self.has_outputs = has_outputs\n",
    "        \n",
    "        self.masks = []\n",
    "        self.X_word = []\n",
    "        self.X_char = []\n",
    "        self.X_lm_inputs = []\n",
    "        self.X_lm_attention = []\n",
    "        self.X_lm_type_ids = []\n",
    "        self.X_lm_spans = []\n",
    "        self.Y_entities = [[] for _ in range(total_layers)]\n",
    "        self.n_skipped = 0\n",
    "        \n",
    "        for i, item in enumerate(data):\n",
    "            if max_items > 0 and i >= max_items:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                x_word = word_input.encode(item['tokens'], padding_length)\n",
    "                x_char = char_input.encode(item['tokens'], padding_length)\n",
    "                x_lm, x_lm_span = lm_input.encode(item['tokens'], padding_length)\n",
    "\n",
    "                mask = [1.] * len(item['tokens']) + [0.] * (padding_length - len(item['tokens']))\n",
    "                self.masks.append(mask)\n",
    "                \n",
    "                self.X_word.append(x_word)\n",
    "                self.X_char.append(x_char)\n",
    "\n",
    "                self.X_lm_inputs.append(x_lm[0])\n",
    "                self.X_lm_attention.append(x_lm[1])\n",
    "                self.X_lm_type_ids.append(x_lm[2])\n",
    "                self.X_lm_spans.append(x_lm_span)\n",
    "                \n",
    "                if has_outputs:\n",
    "                    for i_layer, raw_seq in enumerate(item['layer_outputs']):\n",
    "                        if i_layer >= total_layers:\n",
    "                            break\n",
    "                        padded_seq = raw_seq if len(raw_seq) > 0 else []\n",
    "                        if len(padded_seq) < padding_length - i_layer:\n",
    "                            padded_seq = padded_seq + [0] * (padding_length - len(padded_seq) - i_layer)\n",
    "                        self.Y_entities[i_layer].append(padded_seq)\n",
    "            except Exception as e:\n",
    "                # Text-too-long exception\n",
    "                if skip_exceptions:\n",
    "                    self.n_skipped += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        self._init_tensors(padding_length, total_layers)\n",
    "\n",
    "    def _init_tensors(self, padding_length, total_layers):\n",
    "        self.masks = torch.tensor(self.masks, dtype=torch.float)\n",
    "        self.X_word = torch.tensor(self.X_word, dtype=torch.long)\n",
    "        self.X_char = torch.tensor(self.X_char, dtype=torch.long)\n",
    "        self.X_lm_inputs = torch.tensor(self.X_lm_inputs, dtype=torch.long)\n",
    "        self.X_lm_attention = torch.tensor(self.X_lm_attention, dtype=torch.long)\n",
    "        self.X_lm_type_ids = torch.tensor(self.X_lm_type_ids, dtype=torch.long)\n",
    "        self.X_lm_spans = torch.tensor(self.X_lm_spans, dtype=torch.long)\n",
    "        if self.has_outputs:\n",
    "            for i, seqs in enumerate(self.Y_entities):\n",
    "                self.Y_entities[i] = torch.tensor(seqs, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_word)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {\n",
    "            'masks': self.masks[idx],\n",
    "            'x_word': self.X_word[idx],\n",
    "            'x_char': self.X_char[idx],\n",
    "            'x_lm_input': self.X_lm_inputs[idx],\n",
    "            'x_lm_attention': self.X_lm_attention[idx],\n",
    "            'x_lm_type_ids': self.X_lm_type_ids[idx],\n",
    "            'x_lm_spans': self.X_lm_spans[idx],\n",
    "        }\n",
    "        if self.has_outputs:\n",
    "            for i_layer in range(self.total_layers):\n",
    "                data['y_target_%d' % i_layer] = self.Y_entities[i_layer][idx]\n",
    "        return data\n",
    "\n",
    "    def get_item(self, idx):\n",
    "        return self.__getitem__(idx)\n",
    "    \n",
    "    def get_n_skipped(self):\n",
    "        return self.n_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nne_train_dataset = NestedNamedEntitiesDataset(train_dataset, word_input, char_input, bert_input,\n",
    "                                               total_layers=TOTAL_LAYERS, skip_exceptions=False, max_items=-1)\n",
    "\n",
    "print('Skipped: %d' % nne_train_dataset.get_n_skipped())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample item\n",
    "sample_item = nne_train_dataset.get_item(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n",
    "As it is described in the paper, here are the hyperparameters to be used:\n",
    "\n",
    "![../images/hyperparameters.jpg](../images/hyperparameters.jpg)\n",
    "\n",
    "Note that the authors increased by $0.05$ the dropout rates: \"*\\[...\\] with pre-trained contextualized embeddings, \\[...\\] we increase the dropout rate by 0.05 for these settings.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embeddings(input_embedding):\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform_(input_embedding, -bias, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear(input_linear):\n",
    "    \"\"\"\n",
    "    Initialize linear transformation\n",
    "    \"\"\"\n",
    "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
    "    nn.init.uniform_(input_linear.weight, -bias, bias)\n",
    "    if input_linear.bias is not None:\n",
    "        input_linear.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm(input_lstm):\n",
    "    \"\"\"\n",
    "    Initialize lstm\n",
    "    \n",
    "    PyTorch weights parameters:\n",
    "    \n",
    "        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
    "            of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is\n",
    "            `(hidden_size * hidden_size)`\n",
    "            \n",
    "        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
    "            of shape `(hidden_size * hidden_size)`            \n",
    "    \"\"\"\n",
    "    # Weights init for forward layer\n",
    "    for ind in range(0, input_lstm.num_layers):\n",
    "        \n",
    "        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n",
    "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "        \n",
    "        # Initialize the sampling range\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        \n",
    "        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n",
    "        nn.init.uniform_(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        # Similar to above but for the hidden-hidden weights of the current layer\n",
    "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform_(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        \n",
    "    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n",
    "    if input_lstm.bidirectional:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform_(weight, -sampling_range, sampling_range)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform_(weight, -sampling_range, sampling_range)\n",
    "\n",
    "    # Bias initialization steps\n",
    "    \n",
    "    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n",
    "    if input_lstm.bias:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "            \n",
    "            # Initializing to zero\n",
    "            bias.data.zero_()\n",
    "            \n",
    "            # This is the range of indices for our forget gates for each LSTM cell\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "            #Similar for the hidden-hidden layer\n",
    "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n",
    "        if input_lstm.bidirectional:\n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "Formally, given an input sentence $x$:\n",
    "\n",
    "$$\n",
    "\\tilde{x}_{char} = LSTM_{char}(Embed_{char}(x)) \\\\\n",
    "\\tilde{x}_{word} = Embed_{word}(x) \\\\\n",
    "\\tilde{x} = LSTM_{enc}([\\tilde{x}_{char}; \\tilde{x}_{word}])\n",
    "$$\n",
    "\n",
    "Then, we concatenate the previous result with the output of the Language Model:\n",
    "$$\\tilde{x} = Linear_{enc}([\\tilde{x}; LM(x)])$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(trainable, embedding_matrix=None, shape=None, device=None):\n",
    "    if embedding_matrix is None:\n",
    "        emb_layer = nn.Embedding(num_embeddings=shape[0], embedding_dim=shape[1])\n",
    "        emb_layer.weight.requires_grad = trainable\n",
    "    else:\n",
    "        embedding_tensor = torch.FloatTensor(embedding_matrix)\n",
    "        emb_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=(not trainable))\n",
    "    return emb_layer.to(device=device)\n",
    "    \n",
    "def create_lm_layer(lm_name, trainable, device=None, artifacts_path='../artifacts'):\n",
    "    lm_path = '%s/%s/' % (artifacts_path, lm_name)\n",
    "    bert_model = BertModel.from_pretrained(lm_path)\n",
    "    if not trainable:\n",
    "        for param in bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    return bert_model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEncoder(nn.Module):\n",
    "    def __init__(self, char2id, dimension=60, hidden_size=100, device=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.embedding = create_emb_layer(True, shape=(len(char2id), dimension), device=device)\n",
    "        init_embeddings(self.embedding.weight)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=dimension, hidden_size=hidden_size,\n",
    "                            bidirectional=True, batch_first=True).to(device=device)\n",
    "        init_lstm(self.lstm)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        \n",
    "        outputs = []\n",
    "        for seq in x:\n",
    "            x_output, _ = self.lstm(seq)\n",
    "            outputs.append(x_output[:, -1])\n",
    "\n",
    "        return torch.stack(outputs).to(device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "char_encoder = CharEncoder(char2id, dimension=60, hidden_size=100, device=device)\n",
    "\n",
    "sample_item = nne_train_dataset.get_item(0)\n",
    "sample_x_char = torch.unsqueeze(sample_item['x_char'], 0).to(device=device)\n",
    "ce_output = char_encoder(sample_x_char)\n",
    "print(ce_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMEncoder(nn.Module):\n",
    "    def __init__(self, lm_name, device=None):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.lm_layer = create_lm_layer(lm_name, False, device=device)\n",
    "    \n",
    "    def _create_lm_layer(self, lm_name, trainable):\n",
    "        bert_model = BertModel.from_pretrained(lm_name)\n",
    "        if not trainable:\n",
    "            for param in bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        return bert_model.to(device=self.device)\n",
    "\n",
    "    def forward(self, inputs, attention, type_ids, lm_spans, masks):\n",
    "        x_lm = self.lm_layer(input_ids=inputs, attention_mask=attention, token_type_ids=type_ids,\n",
    "                             output_hidden_states=True)\n",
    "        x_lm = torch.stack(x_lm[2][-4:])\n",
    "        x_lm = torch.mean(x_lm, dim=0)\n",
    "\n",
    "        x = torch.zeros(size=x_lm.size(), device=self.device)\n",
    "        for seq_i, seq_span in enumerate(lm_spans):\n",
    "            mask_length = masks[seq_i].sum()\n",
    "            \n",
    "            for token_i, span in enumerate(seq_span):\n",
    "                if token_i >= mask_length - 1:\n",
    "                    # Skips from SEP token\n",
    "                    break\n",
    "                elif token_i == 0:\n",
    "                    # Skips CLS token\n",
    "                    continue\n",
    "                \n",
    "                token_i -= 1\n",
    "                for k in range(span):\n",
    "                    x[seq_i, token_i] = x[seq_i, token_i].add(x_lm[seq_i, token_i+k+1])\n",
    "                x[seq_i, token_i].div(span)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "encoder_layer = LMEncoder(LM_NAME, device=device)\n",
    "\n",
    "sample_data = nne_train_dataset.get_item(0)\n",
    "masks = torch.unsqueeze(sample_data['masks'], 0).to(device=device)\n",
    "x_lm_inputs = torch.unsqueeze(sample_data['x_lm_input'], 0).to(device=device)\n",
    "x_lm_attention = torch.unsqueeze(sample_data['x_lm_attention'], 0).to(device=device)\n",
    "x_lm_type_ids = torch.unsqueeze(sample_data['x_lm_type_ids'], 0).to(device=device)\n",
    "x_lm_span = torch.unsqueeze(sample_data['x_lm_spans'], 0).to(device=device)\n",
    "\n",
    "out_encoder = encoder_layer(x_lm_inputs, x_lm_attention, x_lm_type_ids, x_lm_span, masks)\n",
    "out_encoder.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, lm_name, word_embeddings, char2id, char_dimension=60, word_dimension=200,\n",
    "                 lm_dimension=1024, hidden_size=100, drop_rate=0.45, device=None):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        self.char_encoder = CharEncoder(char2id, char_dimension, hidden_size, device=device)\n",
    "        self.emb_word = self._create_emb_layer(False, embedding_matrix=word_embeddings, device=device).to(device=device)\n",
    "        self.dropout = nn.Dropout(drop_rate).to(device=device)\n",
    "        self.lm_encoder = LMEncoder(lm_name, device=device)\n",
    "\n",
    "        self.lstm_char = nn.LSTM(input_size=char_dimension, hidden_size=hidden_size,\n",
    "                                 bidirectional=True, batch_first=True).to(device=device)\n",
    "        init_lstm(self.lstm_char)\n",
    "        \n",
    "        self.lstm_enc = nn.LSTM(input_size=(hidden_size*2 + word_dimension),\n",
    "                                hidden_size=hidden_size, bidirectional=True, batch_first=True).to(device=device)\n",
    "        init_lstm(self.lstm_enc)\n",
    "        \n",
    "        self.linear = nn.Linear(lm_dimension + hidden_size*2, hidden_size*2).to(device=device)\n",
    "        init_linear(self.linear)\n",
    "\n",
    "    def forward(self, input_word, input_char, input_lm, input_lm_attention, input_lm_type_ids,\n",
    "                input_lm_spans, input_masks):\n",
    "        x_char = self.char_encoder(input_char)\n",
    "        x_word = self.emb_word(input_word)\n",
    "        \n",
    "        x_enc = torch.cat((x_char, x_word), dim=-1)\n",
    "        x_enc = self.dropout(x_enc)\n",
    "        x_enc, _ = self.lstm_enc(x_enc)\n",
    "        \n",
    "        x_lm = self.lm_encoder(input_lm, input_lm_attention, input_lm_type_ids, input_lm_spans, input_masks)\n",
    "        \n",
    "        x = torch.cat((x_enc, x_lm), dim=2)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _create_emb_layer(self, trainable, embedding_matrix=None, shape=None, device=None):\n",
    "        if embedding_matrix is None:\n",
    "            emb_layer = nn.Embedding(num_embeddings=shape[0], embedding_dim=shape[1])\n",
    "            emb_layer.weight.requires_grad = trainable\n",
    "        else:\n",
    "            embedding_tensor = torch.FloatTensor(embedding_matrix).to(device=self.device)\n",
    "            emb_layer = nn.Embedding.from_pretrained(embedding_tensor, freeze=(not trainable))\n",
    "        return emb_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "encoder_layer = EncoderLayer(LM_NAME, embedding_matrix, char2id, char_dimension=60,\n",
    "                             word_dimension=WORD_DIM, lm_dimension=LM_DIM,\n",
    "                             hidden_size=100, drop_rate=0.45, device=device)\n",
    "\n",
    "sample_data = nne_train_dataset.get_item(0)\n",
    "masks = torch.unsqueeze(sample_data['masks'], 0).to(device=device)\n",
    "x_word = torch.unsqueeze(sample_data['x_word'], 0).to(device=device)\n",
    "x_char = torch.unsqueeze(sample_data['x_char'], 0).to(device=device)\n",
    "x_lm_inputs = torch.unsqueeze(sample_data['x_lm_input'], 0).to(device=device)\n",
    "x_lm_attention = torch.unsqueeze(sample_data['x_lm_attention'], 0).to(device=device)\n",
    "x_lm_type_ids = torch.unsqueeze(sample_data['x_lm_type_ids'], 0).to(device=device)\n",
    "x_lm_span = torch.unsqueeze(sample_data['x_lm_spans'], 0).to(device=device)\n",
    "\n",
    "out_encoder = encoder_layer(x_word, x_char, x_lm_inputs, x_lm_attention, x_lm_type_ids, x_lm_span, masks)\n",
    "out_encoder.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyramid\n",
    "\n",
    "The first pyramid is composed of several decoding layers:\n",
    "\n",
    "![../images/decoding_layer.jpg](../images/decoding_layer.jpg)\n",
    "\n",
    "The paper does not contain information about the hyperparameters of the LSTM layer, so I assume the same hidden space as previous.\n",
    "\n",
    "Regarding the convolutional layer, the authors say \"*\\[...\\] CNN with a kernel of two \\[...\\] and the CNN aggregates two adjacent hidden states \\[...\\]*\". I assume that the authors actually mean $kernel=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodingLayer(nn.Module):\n",
    "    def __init__(self, drop_rate=0.45, seq_length=512, hidden_size=100, device=None):\n",
    "        super(DecodingLayer, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.norm = nn.LayerNorm(normalized_shape=(seq_length, hidden_size*2)).to(device=device)\n",
    "        self.dropout_1 = nn.Dropout(drop_rate).to(device=device)\n",
    "        self.dropout_2 = nn.Dropout(drop_rate).to(device=device)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, bidirectional=True, batch_first=True).to(device=device)\n",
    "        self.conv = nn.Conv1d(in_channels=hidden_size*2, out_channels=hidden_size*2, kernel_size=2, stride=1).to(device=device)\n",
    "        \n",
    "        init_lstm(self.lstm)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.norm(input)\n",
    "        x = self.dropout_1(x)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout_2(x)\n",
    "        \n",
    "        h = x\n",
    "        x = x.transpose(2, 1)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        \n",
    "        return h, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "decoding_layer = DecodingLayer(drop_rate=0.45, seq_length=512, hidden_size=100, device=device)\n",
    "\n",
    "out_decoding = decoding_layer(out_encoder)\n",
    "print(out_decoding[0].size(), out_decoding[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidLayer(nn.Module):\n",
    "    def __init__(self, total_layers=16, drop_rate=0.45, seq_length=512, hidden_size=100, device=None):\n",
    "        super(PyramidLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.total_layers = total_layers\n",
    "        self.seq_length = seq_length\n",
    "        self.decoding_layers = nn.ModuleList([DecodingLayer(drop_rate=drop_rate, seq_length=seq_length-i,\n",
    "                                                            hidden_size=hidden_size, device=device) for i in range(total_layers)])\n",
    "    \n",
    "    def forward(self, input):\n",
    "        h = []\n",
    "        x_layer = input\n",
    "        for i, layer in enumerate(self.decoding_layers):\n",
    "            h_layer, x_layer = layer(x_layer)\n",
    "            h.append(h_layer)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid_layer = PyramidLayer(total_layers=3, drop_rate=0.45, seq_length=512, hidden_size=100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pyramid = pyramid_layer(out_encoder)\n",
    "\n",
    "for o in out_pyramid:\n",
    "    print(o.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse pyramid\n",
    "\n",
    "The inverse pyramid is composed of several inverse decoding layers:\n",
    "\n",
    "![../images/inverse_decoding_layer.jpg](../images/inverse_decoding_layer.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseDecodingLayer(nn.Module):\n",
    "    def __init__(self, drop_rate=0.45, seq_length=512, hidden_size=100, device=None):\n",
    "        super(InverseDecodingLayer, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.norm = nn.LayerNorm(normalized_shape=(seq_length, hidden_size*2)).to(device=device)\n",
    "        self.dropout_1 = nn.Dropout(drop_rate).to(device=device)\n",
    "        self.dropout_2 = nn.Dropout(drop_rate).to(device=device)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=hidden_size*2, hidden_size=hidden_size, bidirectional=True, batch_first=True).to(device=device)\n",
    "        self.conv = nn.Conv1d(in_channels=hidden_size*4, out_channels=hidden_size*2, kernel_size=2, padding=1, stride=1).to(device=device)\n",
    "        \n",
    "        init_lstm(self.lstm)\n",
    "\n",
    "    def forward(self, input_h, input_x):\n",
    "        x = self.norm(input_x)\n",
    "        x = self.dropout_1(x)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout_2(x)\n",
    "        \n",
    "        x = torch.cat((input_h, x), dim=2)\n",
    "        \n",
    "        h = x\n",
    "        x = x.transpose(2, 1)\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        \n",
    "        return h, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = len(out_pyramid) - 1\n",
    "\n",
    "seq_length = 512\n",
    "hidden_size = 100\n",
    "\n",
    "idecoding_layer = InverseDecodingLayer(drop_rate=0.45, seq_length=seq_length - index, hidden_size=100, device=device)\n",
    "h = out_pyramid[index]\n",
    "x = torch.zeros(h.size()[0], seq_length - index, hidden_size*2).to(device=device)\n",
    "\n",
    "out_idecoding = idecoding_layer(h, x)\n",
    "print(out_idecoding[0].size(), out_idecoding[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_enumerate(L):\n",
    "    i = len(L)\n",
    "    while i > 0:\n",
    "        i -= 1\n",
    "        yield i, L[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InversePyramidLayer(nn.Module):\n",
    "    def __init__(self, total_layers=16, drop_rate=0.45, seq_length=512, hidden_size=100, device=None):\n",
    "        super(InversePyramidLayer, self).__init__()\n",
    "        self.device = device\n",
    "        self.total_layers = total_layers\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.idecoding_layers = nn.ModuleList([InverseDecodingLayer(drop_rate=drop_rate, seq_length=seq_length-i,\n",
    "                                                                    hidden_size=hidden_size, device=device) for i in range(total_layers)])\n",
    "    \n",
    "    def forward(self, input_hs):\n",
    "        h = []\n",
    "        batch_size = input_hs[-1].size()[0]\n",
    "        \n",
    "        x_pad = torch.zeros(batch_size, 1, self.hidden_size*4).to(device=self.device)\n",
    "        x_layer = torch.zeros(batch_size,\n",
    "                              self.seq_length - self.total_layers + 1,\n",
    "                              self.hidden_size*2).to(device=self.device)\n",
    "        \n",
    "        for i, layer in reverse_enumerate(self.idecoding_layers):\n",
    "            h_layer, x_layer = layer(input_hs[i], x_layer)\n",
    "            #x_layer = torch.cat((x_pad, x_layer), dim=1)\n",
    "            h.append(h_layer)\n",
    "        \n",
    "        h.reverse()\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipyramid_layer = InversePyramidLayer(total_layers=3, drop_rate=0.45, seq_length=512, hidden_size=100, device=device)\n",
    "out_ipyramid = ipyramid_layer(out_pyramid)\n",
    "\n",
    "for o in out_ipyramid:\n",
    "    print(o.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all together\n",
    "\n",
    "Finally, we put all the pieces together and add a linear layer to get the class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyramidNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, char_vocab, lm_name='dmis-lab/biobert-large-cased-v1.1',\n",
    "                 total_layers=16, drop_rate=0.45, seq_length=512, char_dimension=60, word_dimension=200,\n",
    "                 lm_dimension=1024, hidden_size=100, total_classes=10, device=None):\n",
    "        super(PyramidNet, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder_layer = EncoderLayer(\n",
    "                lm_name, embedding_matrix, char_vocab, char_dimension=char_dimension,\n",
    "                word_dimension=word_dimension, lm_dimension=lm_dimension, hidden_size=hidden_size,\n",
    "                drop_rate=drop_rate, device=device)\n",
    "        \n",
    "        self.pyramid = PyramidLayer(total_layers=total_layers, drop_rate=drop_rate,\n",
    "                                    seq_length=seq_length, hidden_size=hidden_size, device=device)\n",
    "        self.inverse_pyramid = InversePyramidLayer(total_layers=total_layers, drop_rate=drop_rate,\n",
    "                                                   seq_length=seq_length, hidden_size=hidden_size, device=device)\n",
    "        self.linear = nn.Linear(hidden_size*4, total_classes).to(device=self.device)\n",
    "    \n",
    "    def forward(self, input_word, input_char, input_lm, input_lm_attention,\n",
    "                input_lm_type_ids, input_lm_spans, input_masks):\n",
    "        x = self.encoder_layer(input_word, input_char, input_lm, input_lm_attention,\n",
    "                               input_lm_type_ids, input_lm_spans, input_masks)\n",
    "        x = self.pyramid(x)\n",
    "        x = self.inverse_pyramid(x)\n",
    "        x = [self.linear(x_layer) for x_layer in x]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "net = PyramidNet(embedding_matrix, char2id, lm_name=LM_NAME, total_layers=5,\n",
    "                 drop_rate=0.4, seq_length=512, char_dimension=60, word_dimension=WORD_DIM,\n",
    "                 lm_dimension=LM_DIM, total_classes=10, device=device)\n",
    "\n",
    "sample_data = nne_train_dataset.get_item(0)\n",
    "masks = torch.unsqueeze(sample_data['masks'], 0).to(device=device)\n",
    "x_word = torch.unsqueeze(sample_data['x_word'], 0).to(device=device)\n",
    "x_char = torch.unsqueeze(sample_data['x_char'], 0).to(device=device)\n",
    "x_lm_inputs = torch.unsqueeze(sample_data['x_lm_input'], 0).to(device=device)\n",
    "x_lm_attention = torch.unsqueeze(sample_data['x_lm_attention'], 0).to(device=device)\n",
    "x_lm_type_ids = torch.unsqueeze(sample_data['x_lm_type_ids'], 0).to(device=device)\n",
    "x_lm_span = torch.unsqueeze(sample_data['x_lm_spans'], 0).to(device=device)\n",
    "\n",
    "out_net = net(x_word, x_char, x_lm_inputs, x_lm_attention, x_lm_type_ids, x_lm_span, masks)\n",
    "for o in out_net:\n",
    "    print(o.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For training, we should use inverse time learning rate decay:\n",
    "\n",
    "$$\\widehat{lr} = \\frac{lr}{1 + \\text{decay_rate} \\cdot \\text{steps}\\ /\\ \\text{decay_steps}}$$\n",
    "\n",
    "As stated by the authors, we will use $0.05$ and $1000$ for the decay rate and decay steps respectively. The rest of hyperparameters are described in the Table 2 of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "INITIAL_LR = 0.01\n",
    "DECAY_RATE = 0.05\n",
    "DECAY_STEPS = 1000\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 8 # 64\n",
    "GRADIENT_CLIP = 5.\n",
    "EPOCHS = 10 # Not defined in the paper\n",
    "\n",
    "# For debug purposes, force stop after X steps (-1 if disabled)\n",
    "STOP_AFTER = -1\n",
    "\n",
    "# Hyperparameters\n",
    "DROP_RATE = 0.45\n",
    "CHAR_DIM = 60\n",
    "WORD_DIM = 100 # 200\n",
    "HIDDEN_SIZE = 100\n",
    "TOTAL_CLASSES = 10\n",
    "\n",
    "ARTIFACTS_PATH = '../artifacts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, step, decay_rate=DECAY_RATE, decay_steps=DECAY_STEPS, inital_lr=INITIAL_LR):\n",
    "    \"\"\"\n",
    "    Ajusts Learnin-Rate using the formula described in the paper\n",
    "    \"\"\"\n",
    "    lr = inital_lr / (1 + decay_rate * step / decay_steps)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PyramidNet(embedding_matrix, char2id, lm_name=LM_NAME, total_layers=TOTAL_LAYERS,\n",
    "                 drop_rate=DROP_RATE, char_dimension=CHAR_DIM, word_dimension=WORD_DIM,\n",
    "                 lm_dimension=LM_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                 total_classes=TOTAL_CLASSES, device=device)\n",
    "\n",
    "net = net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder: this criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(nne_train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "step = 0\n",
    "n_batches = len(train_dataloader)\n",
    "\n",
    "print('Starting...', end='\\r')\n",
    "\n",
    "for i_epoch in range(EPOCHS):\n",
    "    run_loss = 0\n",
    "\n",
    "    for i_batch, batch_data in enumerate(train_dataloader):\n",
    "        step += 1\n",
    "\n",
    "        # Get inputs\n",
    "        masks = batch_data['masks'].to(device=device)\n",
    "        x_word = batch_data['x_word'].to(device=device)\n",
    "        x_char = batch_data['x_char'].to(device=device)\n",
    "        x_lm_inputs = batch_data['x_lm_input'].to(device=device)\n",
    "        x_lm_attention = batch_data['x_lm_attention'].to(device=device)\n",
    "        x_lm_type_ids = batch_data['x_lm_type_ids'].to(device=device)\n",
    "        x_lm_span = batch_data['x_lm_spans'].to(device=device)\n",
    "        \n",
    "        y_all_targets = []\n",
    "        for i_layer in range(TOTAL_LAYERS):\n",
    "            y_target = batch_data['y_target_%d' % i_layer].to(device=device)\n",
    "            y_all_targets.append(y_target)\n",
    "\n",
    "        # Predict entities\n",
    "        y_all_preds = net(x_word, x_char, x_lm_inputs, x_lm_attention, x_lm_type_ids, x_lm_span, masks)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = 0\n",
    "        for i_pred, y_pred_logits in enumerate(y_all_preds):\n",
    "            loss_tensor = criterion(y_pred_logits.permute(0, -1, 1), y_all_targets[i_pred])\n",
    "            loss += (loss_tensor * masks[:,i_pred:]).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(net.parameters(), GRADIENT_CLIP) # Avoid gradient exploding issue\n",
    "        optimizer.step()\n",
    "        \n",
    "        adjust_lr(optimizer, step)\n",
    "\n",
    "        run_loss += loss.cpu().data.numpy()\n",
    "        print(\"Epoch %d of %d | Batch %d of %d | Loss = %.3f\" % (i_epoch + 1, EPOCHS, i_batch + 1, n_batches, run_loss / (i_batch + 1)),\n",
    "              ' ' * 10,\n",
    "              end='\\r')\n",
    "        \n",
    "        if STOP_AFTER != -1 and STOP_AFTER <= step:\n",
    "            break\n",
    "        \n",
    "        # Clear some memory\n",
    "        if device == 'cuda':\n",
    "            del masks\n",
    "            del x_word\n",
    "            del x_char\n",
    "            del x_lm_inputs\n",
    "            del x_lm_attention\n",
    "            del x_lm_type_ids\n",
    "            del x_lm_span\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    history.append(run_loss / len(train_dataloader))\n",
    "    print(\"Epoch %d of %d | Loss = %.3f\" % (i_epoch + 1, EPOCHS, run_loss / len(train_dataloader)), ' ' * 20)\n",
    "    \n",
    "    if STOP_AFTER != -1 and STOP_AFTER <= step:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "filepath = '%s%s' % (ARTIFACTS_PATH, 'genia_model.pt')\n",
    "torch.save(net.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation per layer\n",
    "\n",
    "Basic evaluation of the outputs of each layer. This implementation takes the output sequence as a whole (i.e. including all IOB tags), and computes the usual scores (precision, recall and F1-score) comparing those sequences with the real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "net = PyramidNet(embedding_matrix, char2id, lm_name=LM_NAME, total_layers=TOTAL_LAYERS,\n",
    "                 drop_rate=DROP_RATE, char_dimension=CHAR_DIM, word_dimension=WORD_DIM,\n",
    "                 lm_dimension=LM_DIM, hidden_size=HIDDEN_SIZE,\n",
    "                 total_classes=TOTAL_CLASSES, device=device)\n",
    "net = net.to(device=device)\n",
    "\n",
    "filepath = '%s%s' % (ARTIFACTS_PATH, 'genia_model.pt')\n",
    "net.load_state_dict(torch.load(filepath))\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nne_test_dataset = NestedNamedEntitiesDataset(test_dataset, word_input, char_input, bert_input,\n",
    "                                              total_layers=TOTAL_LAYERS, skip_exceptions=False, max_items=-1)\n",
    "\n",
    "print('Skipped: %d' % nne_train_dataset.get_n_skipped())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(nne_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(range(TOTAL_CLASSES))\n",
    "precisions = [[] for _ in range(TOTAL_LAYERS)]\n",
    "recalls = [[] for _ in range(TOTAL_LAYERS)]\n",
    "\n",
    "for i_batch, batch_data in enumerate(test_dataloader):\n",
    "    print('Evaluating batch %d out of %d' % (i_batch+1, len(test_dataloader)), end='\\r')\n",
    "    \n",
    "    # Get inputs\n",
    "    masks = batch_data['masks'].to(device=device)\n",
    "    x_word = batch_data['x_word'].to(device=device)\n",
    "    x_char = batch_data['x_char'].to(device=device)\n",
    "    x_lm_inputs = batch_data['x_lm_input'].to(device=device)\n",
    "    x_lm_attention = batch_data['x_lm_attention'].to(device=device)\n",
    "    x_lm_type_ids = batch_data['x_lm_type_ids'].to(device=device)\n",
    "    x_lm_span = batch_data['x_lm_spans'].to(device=device)\n",
    "\n",
    "    y_all_preds = net(x_word, x_char, x_lm_inputs, x_lm_attention, x_lm_type_ids, x_lm_span, masks)\n",
    "    \n",
    "    y_all_targets = []\n",
    "    for i_layer in range(TOTAL_LAYERS):\n",
    "        y_targets = batch_data['y_target_%d' % i_layer]\n",
    "        y_preds = y_all_preds[i_layer].cpu().detach()\n",
    "        \n",
    "        for (y_target, y_pred, mask) in zip(y_targets, y_preds, masks):\n",
    "            mask_cut = int(mask[i_layer:].sum().cpu().detach().numpy())\n",
    "            y_target = y_target.view(-1)[:mask_cut]\n",
    "            y_pred = torch.argmax(y_pred, dim=-1).view(-1)[:mask_cut]\n",
    "            \n",
    "            p_score = precision_score(y_target, y_pred, labels=labels, average='micro')\n",
    "            precisions[i_layer].append(p_score)\n",
    "\n",
    "            r_score = recall_score(y_target, y_pred, labels=labels, average='micro')\n",
    "            recalls[i_layer].append(r_score)\n",
    "    \n",
    "    # Clear some memory\n",
    "    if device == 'cuda':\n",
    "        del masks\n",
    "        del x_word\n",
    "        del x_char\n",
    "        del x_lm_inputs\n",
    "        del x_lm_attention\n",
    "        del x_lm_type_ids\n",
    "        del x_lm_span\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SCORES PER LAYER')\n",
    "for i_layer in range(TOTAL_LAYERS):\n",
    "    print('- Layer %d - Precision: %.4f - Recall: %.4f' % (i_layer + 1,\n",
    "                                                           np.mean(precisions[i_layer]),\n",
    "                                                           np.mean(recalls[i_layer])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall evaluation\n",
    "\n",
    "The following evaluation is based on the original author's implementation (see [Github](https://github.com/LorrinWWW/Pyramid/blob/7c63639df7a6fddc19730af98c37be22dec01221/utils/data.py#L103)). It evaluates the usual scores (precision, recall and F1-score) for exact predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2span(seq, return_types=False, entity_idx=None):\n",
    "    if entity_idx is not None:\n",
    "        seq = [entity_idx[x] for x in seq]\n",
    "\n",
    "    spans = []\n",
    "    types = []\n",
    "    _span = _type = None\n",
    "    for i, t in enumerate(seq):\n",
    "        if (t[0] == 'B' or t == 'O') and _span is not None:\n",
    "            spans.append(_span)\n",
    "            types.append(_type)\n",
    "            _span = _type = None\n",
    "        if t[0] == 'B':\n",
    "            _span = [i, i+1]\n",
    "            _type = t[2:]\n",
    "        if t[0] == 'I':\n",
    "            if _span is not None:\n",
    "                _span[1] = i+1\n",
    "\n",
    "    if _span is not None:\n",
    "        spans.append(_span)\n",
    "        types.append(_type)\n",
    "        \n",
    "    if return_types:\n",
    "        return spans, types\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_metrics(labels, preds, entity_idx, verbose=0):\n",
    "    n_correct = n_recall = n_precision = 0\n",
    "    confusion_dict = defaultdict(lambda: [0, 0, 0]) # n_correct, n_preds, n_labels\n",
    "    for i in range(len(labels)):\n",
    "        if verbose > 0:\n",
    "            print('Evaluating %d out of %d' % (i+1, len(labels)), end='\\r')\n",
    "        \n",
    "        i_label = labels[i]\n",
    "        i_pred = preds[i][:len(i_label)]\n",
    "\n",
    "        spans, types = seq2span(i_pred, True, entity_idx)\n",
    "        pred_set = {(_type, _span[0], _span[1]) for _span, _type in zip(spans, types)}\n",
    "\n",
    "        spans, types = seq2span(i_label, True, entity_idx)\n",
    "        label_set = {(_type, _span[0], _span[1]) for _span, _type in zip(spans, types)}\n",
    "\n",
    "        correct_set = pred_set & label_set\n",
    "        \n",
    "        for _type, _, _ in correct_set:\n",
    "            confusion_dict[_type][0] += 1\n",
    "        for _type, _, _ in pred_set:\n",
    "            confusion_dict[_type][1] += 1\n",
    "        for _type, _, _ in label_set:\n",
    "            confusion_dict[_type][2] += 1\n",
    "\n",
    "        n_correct += len(correct_set)\n",
    "        n_recall += len(label_set)\n",
    "        n_precision += len(pred_set)\n",
    "    \n",
    "    try:\n",
    "        recall = n_correct / n_recall\n",
    "        precision = n_correct / n_precision\n",
    "        f1 = 2 / (1/recall + 1/precision)\n",
    "    except:\n",
    "        recall = precision = f1 = 0\n",
    "\n",
    "    if verbose > 0:\n",
    "        print()\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_dict': confusion_dict,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_labels = []\n",
    "seq_preds = []\n",
    "\n",
    "for i_batch, batch_data in enumerate(test_dataloader):\n",
    "    print('Getting pred. of batch %d out of %d' % (i_batch+1, len(test_dataloader)), end='\\r')\n",
    "    \n",
    "    # Get inputs\n",
    "    masks = batch_data['masks'].to(device=device)\n",
    "    x_word = batch_data['x_word'].to(device=device)\n",
    "    x_char = batch_data['x_char'].to(device=device)\n",
    "    x_lm_inputs = batch_data['x_lm_input'].to(device=device)\n",
    "    x_lm_attention = batch_data['x_lm_attention'].to(device=device)\n",
    "    x_lm_type_ids = batch_data['x_lm_type_ids'].to(device=device)\n",
    "    x_lm_span = batch_data['x_lm_spans'].to(device=device)\n",
    "\n",
    "    y_all_preds = net(x_word, x_char, x_lm_inputs, x_lm_attention, x_lm_type_ids, x_lm_span, masks)\n",
    "    \n",
    "    y_all_targets = []\n",
    "    \n",
    "    for i_layer in range(TOTAL_LAYERS):\n",
    "        y_targets = batch_data['y_target_%d' % i_layer]\n",
    "        y_preds = y_all_preds[i_layer].cpu().detach()\n",
    "        \n",
    "        for (y_target, y_pred, mask) in zip(y_targets, y_preds, masks):\n",
    "            mask_cut = int(mask[i_layer:].sum().cpu().detach().numpy())\n",
    "            y_target = y_target.view(-1)[:mask_cut]\n",
    "            y_pred = torch.argmax(y_pred, dim=-1).view(-1)[:mask_cut]\n",
    "            \n",
    "            seq_labels.append(y_target.cpu().detach().numpy())\n",
    "            seq_preds.append(y_pred.cpu().detach().numpy())\n",
    "    \n",
    "    # Clear some memory\n",
    "    if device == 'cuda':\n",
    "        del masks\n",
    "        del x_word\n",
    "        del x_char\n",
    "        del x_lm_inputs\n",
    "        del x_lm_attention\n",
    "        del x_lm_type_ids\n",
    "        del x_lm_span\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_scores = get_seq_metrics(seq_labels, seq_preds, entity_idx, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OVERALL SCORES')\n",
    "print('- Precision: %.4f' % overall_scores['precision'])\n",
    "print('- Recall: %.4f' % overall_scores['recall'])\n",
    "print('- F1-score: %.4f' % overall_scores['f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox - Try it yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = 'Two cDNA clones were sequenced and provided 2,250 nucleotides (nt) of DNA sequence information.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, spans = tokenize_text(tokenizer, my_text, True)\n",
    "eval_data = [{'tokens': tokens}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nne_eval_dataset = NestedNamedEntitiesDataset(eval_data, word_input, char_input, bert_input, skip_exceptions=False,\n",
    "                                              max_items=-1, total_layers=TOTAL_LAYERS, has_outputs=False)\n",
    "eval_dataloader = DataLoader(nne_eval_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_item = next(iter(eval_dataloader))\n",
    "\n",
    "masks = eval_item['masks'].to(device=device)\n",
    "x_word = eval_item['x_word'].to(device=device)\n",
    "x_char = eval_item['x_char'].to(device=device)\n",
    "x_lm_inputs = eval_item['x_lm_input'].to(device=device)\n",
    "x_lm_attention = eval_item['x_lm_attention'].to(device=device)\n",
    "x_lm_type_ids = eval_item['x_lm_type_ids'].to(device=device)\n",
    "x_lm_span = eval_item['x_lm_spans'].to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all_preds = net(x_word, x_char, x_lm_inputs, x_lm_attention, x_lm_type_ids, x_lm_span, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_entities = []\n",
    "\n",
    "for i_layer in range(TOTAL_LAYERS):\n",
    "    y_pred = y_all_preds[i_layer].cpu().detach()\n",
    "    y_pred = torch.argmax(y_pred, dim=-1).view(-1)\n",
    "    \n",
    "    for y_span, y in enumerate(y_pred):\n",
    "        if y > 0:\n",
    "            entity = {'span': [y_span, y_span+i_layer],\n",
    "                      'tokens': tokens[y_span:y_span+i_layer+1]}\n",
    "            found_entities.append(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_text)\n",
    "print('-'*10)\n",
    "\n",
    "for entity in found_entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
