{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENIA data\n",
    "\n",
    "Source: http://www.geniaproject.org/genia-corpus/term-corpus\n",
    "\n",
    "For GENIA, the authors use `GENIAcorpus3.02p3`, and follow the train/dev/test split of previous works (Finkel and Manning, 2009; Lu and Roth, 2015) i.e.:\n",
    "1. Split first $81\\%$, subsequent $9\\%$, and last $10\\%$ as train, dev and test set, respectively.\n",
    "2. Collapse all DNA, RNA, and protein subtypes into DNA, RNA, and protein, keeping cell line and cell type.\n",
    "3. Removing other entity types, resulting in 5 entity types.\n",
    "\n",
    "For this reason, we are going to generate three different files: `train.genia.json`, `valid.genia.json` and `test.genia.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions\n",
    "\n",
    "Once we have preprocessed the data, the file should be a list of items where each item looks as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"tokens\": [\"token0\", \"token1\", \"token2\"],\n",
    "  \"entities\": [\n",
    "    {\n",
    "      \"entity_type\": \"PER\", \n",
    "      \"span\": [0, 1],\n",
    "    },\n",
    "    {\n",
    "      \"entity_type\": \"ORG\", \n",
    "      \"span\": [2, 3],\n",
    "    },\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Since we will use GloVe embeddings later, we are going to use the Spacy tokenizer. With this tokenizer, we obtain the same tokens as GloVe (see https://spacy.io/models/en). <u>Note that BERT will use a different tokenizer!</u>\n",
    "\n",
    "You may need to install the English dictionary before using Spacy:\n",
    "`python -m spacy download en_core_web_lg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(artifacts_path='artifacts/', bert_name='dmis-lab/biobert-v1.1'):\n",
    "    slow_tokenizer = BertTokenizer.from_pretrained(bert_name)\n",
    "\n",
    "    save_path = '%s%s/' % (artifacts_path, bert_name)\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        slow_tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    # We can already use the Slow Tokenizer, but its implementation in Rust is much faster.\n",
    "    tokenizer = BertWordPieceTokenizer('%svocab.txt' % save_path, lowercase=True)\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, text, lower=False):\n",
    "    if lower:\n",
    "        text = text.lower()\n",
    "    \n",
    "    encoded = tokenizer.encode(text)\n",
    "    \n",
    "    tokens = encoded.tokens[1:-1]\n",
    "    spans = encoded.offsets[1:-1]\n",
    "    \n",
    "    spans = [[x[0], x[1]-1] for x in spans]\n",
    "    \n",
    "    i = len(tokens)\n",
    "    while i >= 0:\n",
    "        i -= 1\n",
    "        if re.search(r\"^##.+\", tokens[i]):\n",
    "            token = tokens[i][2:]\n",
    "            tokens[i-1] += token\n",
    "            spans[i-1][1] += len(token)\n",
    "            del tokens[i]\n",
    "            del spans[i]\n",
    "\n",
    "    return tokens, spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in [0, 1]\n",
      "primary [3, 9]\n",
      "t [11, 11]\n",
      "lymphocytes [13, 23]\n",
      "we [25, 26]\n",
      "show [28, 31]\n",
      "that [33, 36]\n",
      "cd28 [38, 41]\n",
      "ligation [43, 50]\n",
      "leads [52, 56]\n",
      "\n",
      "to [58, 59]\n",
      "the [61, 63]\n",
      "rapid [65, 69]\n",
      "intracellular [71, 83]\n",
      "formation [85, 93]\n",
      "of [95, 96]\n",
      "reactive [98, 105]\n",
      "oxygen [107, 112]\n",
      "intermediates [114, 126]\n",
      "( [128, 128]\n",
      "\n",
      "rois [129, 132]\n",
      ") [133, 133]\n",
      "which [135, 139]\n",
      "are [141, 143]\n",
      "required [145, 152]\n",
      "for [154, 156]\n",
      "cd28 [158, 161]\n",
      "- [162, 162]\n",
      "mediated [163, 170]\n",
      "activation [172, 181]\n",
      "\n",
      "of [183, 184]\n",
      "the [186, 188]\n",
      "nf [190, 191]\n",
      "- [192, 192]\n",
      "kappa [193, 197]\n",
      "b [199, 199]\n",
      "/ [200, 200]\n",
      "cd28 [201, 204]\n",
      "- [205, 205]\n",
      "responsive [206, 215]\n",
      "\n",
      "complex [217, 223]\n",
      "and [225, 227]\n",
      "il [229, 230]\n",
      "- [231, 231]\n",
      "2 [232, 232]\n",
      "expression [234, 243]\n",
      ". [244, 244]\n"
     ]
    }
   ],
   "source": [
    "# Debug\n",
    "text = 'In primary T lymphocytes we show that CD28 ligation leads to the rapid intracellular formation of reactive oxygen intermediates (ROIs) which are required for CD28-mediated activation of the NF-kappa B/CD28-responsive complex and IL-2 expression.'\n",
    "\n",
    "i = 1\n",
    "for token, span in zip(*tokenize_text(tokenizer, text, True)):\n",
    "    print(token, span)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inner_xml(element):\n",
    "    return (element.text or '') + ''.join(ET.tostring(e, 'unicode') for e in element)\n",
    "\n",
    "def parse_cons_attributes(match):\n",
    "    attributes = {}\n",
    "    for item in re.findall(r'[a-z]+=\"[^\"]+\"\\s*', match):\n",
    "        m = re.search(r'^([^=]+)=\"([^\"]+)\"', item.strip())\n",
    "        name = m[1]\n",
    "        value = m[2]\n",
    "        attributes[name] = value\n",
    "    return attributes\n",
    "\n",
    "def reverse_enumerate(L):\n",
    "    i = len(L)\n",
    "    while i > 0:\n",
    "        i -= 1\n",
    "        yield i, L[i]\n",
    "\n",
    "def get_raw_entities(xml_sentences):\n",
    "    data = []\n",
    "    regex_open = r'<cons\\s*((?:[a-z]+=\"[^\"]+\"\\s*)*)>'\n",
    "    regex_close = r'</cons>'\n",
    "    \n",
    "    for sentence in xml_sentences:\n",
    "        entities = []\n",
    "        inner_xml = get_inner_xml(sentence)\n",
    "\n",
    "        m_open = re.search(regex_open, inner_xml)\n",
    "        m_close = re.search(regex_close, inner_xml)\n",
    "        while m_open is not None or m_close is not None:\n",
    "            # Check which regex matched first\n",
    "            if m_open or m_close:\n",
    "                \"\"\"\n",
    "                print(inner_xml)\n",
    "                if m_open:\n",
    "                    print('Open:', m_open.span())\n",
    "                if m_close:\n",
    "                    print('Close:', m_close.span())\n",
    "                print('-'*10)\n",
    "                \"\"\"\n",
    "            \n",
    "            if m_close is None or m_open is not None and m_open.span()[0] < m_close.span()[0]:\n",
    "                inner_xml = re.sub(regex_open, '', inner_xml, 1)\n",
    "                cons_attributes = parse_cons_attributes(m_open.group(1))\n",
    "                entities.append({'span': [m_open.span()[0], -1], 'attributes': cons_attributes})\n",
    "            else:\n",
    "                inner_xml = re.sub(regex_close, '', inner_xml, 1)\n",
    "                for _, e in reverse_enumerate(entities):\n",
    "                    # Add close_span to the latest non-closed entity\n",
    "                    if e['span'][1] == -1:\n",
    "                        span_start = e['span'][0]\n",
    "                        e['span'] = [span_start, m_close.span()[0]]\n",
    "                        break\n",
    "\n",
    "            m_open = re.search(regex_open, inner_xml)\n",
    "            m_close = re.search(regex_close, inner_xml)\n",
    "\n",
    "        data.append({'tokens': inner_xml, 'entities': entities})\n",
    "    \n",
    "    return data\n",
    "\n",
    "def split_sem_lex_info(lexsem_types):\n",
    "    lexsem_types = re.sub(r'^\\((AND|OR|AND/OR|AS\\_WELL\\_AS|BUT\\_NOT) ', '', lexsem_types)\n",
    "    lexsem_types = re.sub(r'\\)$', '', lexsem_types)\n",
    "    lexsem_types = lexsem_types.strip().split()\n",
    "    return lexsem_types\n",
    "\n",
    "def expand_nested_attributes(entities):\n",
    "    \"\"\"\n",
    "    Expands 'sem' attribute from the parent entity to the children.\n",
    "    \"\"\"\n",
    "    for e_parent in entities:\n",
    "        if 'sem' not in e_parent['attributes'] or not re.search(r'^\\(AND', e_parent['attributes']['sem']):\n",
    "            continue\n",
    "        \n",
    "        lex_types = split_sem_lex_info(e_parent['attributes']['lex'])\n",
    "        sem_types = split_sem_lex_info(e_parent['attributes']['sem'])\n",
    "        \n",
    "        for i, lex_type in enumerate(lex_types):\n",
    "            for _ in re.findall(r',', lex_type):\n",
    "                sem_types = sem_types[:i] + [sem_types[i]] + sem_types[i:]\n",
    "        \n",
    "        i_sem = 0\n",
    "        for e_child in entities:\n",
    "            if 'sem' in e_child['attributes']:\n",
    "                continue\n",
    "            elif e_parent['span'][0] > e_child['span'][0] or e_parent['span'][1] < e_child['span'][1]:\n",
    "                continue\n",
    "            elif re.search(r'^\\*', e_child['attributes']['lex']):\n",
    "                continue\n",
    "            \n",
    "            if len(sem_types) > i_sem:\n",
    "                e_child['attributes']['sem'] = sem_types[i_sem]\n",
    "            else:\n",
    "                e_child['attributes']['sem'] = 'G#other_name'\n",
    "\n",
    "            i_sem += 1\n",
    "        \n",
    "    return entities\n",
    "\n",
    "def get_entity_type(sem):\n",
    "    if re.search(r'^G\\#other', sem):\n",
    "        return None\n",
    "    elif re.search(r'^G\\#RNA', sem):\n",
    "        return 'RNA'\n",
    "    elif re.search(r'^G\\#DNA', sem):\n",
    "        return 'DNA'\n",
    "    elif re.search(r'^G\\#protein', sem):\n",
    "        return 'protein'\n",
    "    elif re.search(r'^G\\#cell\\_line', sem):\n",
    "        return 'cell_line'\n",
    "    elif re.search(r'^G\\#cell\\_type', sem):\n",
    "        return 'cell_type'\n",
    "    else:\n",
    "        return None\n",
    "        #raise Exception('Unknown sem: %s' % sem)\n",
    "\n",
    "    return sem\n",
    "\n",
    "def get_norm_entities(entities):\n",
    "    \"\"\"\n",
    "    Apply these two preprocess steps:\n",
    "    - Collapses all DNA, RNA, and protein subtypes into DNA, RNA, and protein, keeping cell line and cell type.\n",
    "    - Removes other entity types, resulting in 5 entity types.\n",
    "    \"\"\"\n",
    "    \n",
    "    i = len(entities)\n",
    "    while i > 0:\n",
    "        i -= 1\n",
    "        e = entities[i]\n",
    "\n",
    "        if 'sem' not in e['attributes']:\n",
    "            del entities[i]\n",
    "            continue\n",
    "        \n",
    "        if re.search(r'^\\((AND|OR|AND/OR|AS\\_WELL\\_AS|BUT\\_NOT) ', e['attributes']['sem']):\n",
    "            # Parent entity\n",
    "            sem_types = e['attributes']['sem']\n",
    "            sem_types = re.sub(r'^\\([^\\s]+', '', sem_types)\n",
    "            sem_types = re.sub(r'\\)$', '', sem_types)\n",
    "            sem = sem_types.strip().split()[0]\n",
    "        else:\n",
    "            sem = e['attributes']['sem']\n",
    "        \n",
    "        entity_type = get_entity_type(sem)\n",
    "        if entity_type is None:\n",
    "            # Remove this entity\n",
    "            del entities[i]\n",
    "        else:\n",
    "            del e['attributes']\n",
    "            e['entity_type'] = entity_type\n",
    "\n",
    "    return entities\n",
    "\n",
    "def transform_text_spans(text, entity_list, tokenizer, debug=False):\n",
    "    tokens, spans = tokenize_text(tokenizer, text)\n",
    "    for i, entity in enumerate(entity_list):\n",
    "        if debug:\n",
    "            print('-' * 10)\n",
    "            print('-' * 10)\n",
    "            print(entity)\n",
    "        \n",
    "        span_start, span_end = entity['span']\n",
    "        new_span_start = -1\n",
    "        new_span_end = -1\n",
    "        \n",
    "        j = 0\n",
    "        while j < len(spans):\n",
    "            token_span = spans[j]\n",
    "            if debug:\n",
    "                print('-' * 10)\n",
    "                print('Entity:', span_start, span_end)\n",
    "                print('Token:', token_span)\n",
    "            \n",
    "            if span_start > token_span[0] and span_start < token_span[1]:\n",
    "                # Split token\n",
    "                tokens = tokens[:j] + [tokens[j][:span_start], tokens[j][span_start:]] + tokens[(j+1):]\n",
    "                spans = spans[:j] + [[token_span[0], span_start-1], [span_start, token_span[1]]] + spans[(j+1):]\n",
    "                token_span = spans[j]\n",
    "            \n",
    "            if span_end > token_span[0] and span_end < token_span[1]:\n",
    "                # Split token\n",
    "                tokens = tokens[:j] + [tokens[j][:span_end], tokens[j][span_end:]] + tokens[(j+1):]\n",
    "                spans = spans[:j] + [[token_span[0], span_end-1], [span_end, token_span[1]]] + spans[(j+1):]\n",
    "                token_span = spans[j]\n",
    "\n",
    "            if span_start == token_span[0]:\n",
    "                new_span_start = j\n",
    "            elif span_start == token_span[1]:\n",
    "                new_span_start = j + 1\n",
    "            elif span_start > token_span[1] and len(spans) > j+1 and span_start < spans[j+1][0]:\n",
    "                new_span_start = j + (span_start - token_span[1])\n",
    "\n",
    "            if span_end < token_span[1] and span_end > token_span[0]:\n",
    "                new_span_end = j - (token_span[1] - span_end)\n",
    "                break\n",
    "            elif span_end > token_span[1] and len(spans) > j+1 and span_end < spans[j+1][0]:\n",
    "                new_span_end = j + (span_end - token_span[1])\n",
    "                break\n",
    "            elif span_end == token_span[0]:\n",
    "                new_span_end = j - 1\n",
    "                break\n",
    "            elif span_end == token_span[1]:\n",
    "                new_span_end = j\n",
    "                break\n",
    "            elif span_end > token_span[1] and len(spans) == j+1:\n",
    "                new_span_end = j\n",
    "                break\n",
    "            \n",
    "            j += 1\n",
    "        \n",
    "        if debug:\n",
    "            print('-' * 10)\n",
    "            print(new_span_start, new_span_end)\n",
    "\n",
    "        if new_span_start != -1 and new_span_end != -1:\n",
    "            entity['span'] = [new_span_start, new_span_end]\n",
    "\n",
    "    return tokens, entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_genia(filepath, tokenizer, total_layers, no_entity='O'):\n",
    "    \"\"\"\n",
    "    Main function to parse the GENIA corpus.\n",
    "    \n",
    "    Arguments:\n",
    "    - filepath (str): path to the GENIAcorpus3.02.xml file.\n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    \"\"\"\n",
    "    genia_data = []\n",
    "    root = ET.parse(filepath).getroot()\n",
    "\n",
    "    #xml = '<set><article><articleinfo><bibliomisc>MEDLINE:98208270</bibliomisc></articleinfo><title></title><abstract><sentence>In <cons lex=\"primary_T_lymphocyte\" sem=\"G#cell_type\">primary T lymphocytes</cons> we show that <cons lex=\"CD28\" sem=\"G#protein_molecule\">CD28</cons> ligation leads to the rapid intracellular formation of <cons lex=\"reactive_oxygen_intermediate\" sem=\"G#inorganic\">reactive oxygen intermediates</cons> (<cons lex=\"ROI\" sem=\"G#inorganic\">ROIs</cons>) which are required for <cons lex=\"CD28-mediated_activation\" sem=\"G#other_name\"><cons lex=\"CD28\" sem=\"G#protein_molecule\">CD28</cons>-mediated activation</cons> of the <cons lex=\"NF-kappa_B\" sem=\"G#protein_molecule\">NF-kappa B</cons>/<cons lex=\"CD28-responsive_complex\" sem=\"G#protein_complex\"><cons lex=\"CD28\" sem=\"G#protein_molecule\">CD28</cons>-responsive complex</cons> and <cons lex=\"IL-2_expression\" sem=\"G#other_name\"><cons lex=\"IL-2\" sem=\"G#protein_molecule\">IL-2</cons> expression</cons>.</sentence></abstract></article></set>'\n",
    "    #root = ET.fromstring(xml)\n",
    "    \n",
    "    for child in root:\n",
    "        if child.tag != 'article':\n",
    "            continue\n",
    "\n",
    "        child_data = get_raw_entities(child.find('title').findall('sentence'))\n",
    "        child_data += get_raw_entities(child.find('abstract').findall('sentence'))\n",
    "\n",
    "        for c in child_data:\n",
    "            c['entities'] = expand_nested_attributes(c['entities'])\n",
    "            c['entities'] = get_norm_entities(c['entities'])\n",
    "            \n",
    "            c['text'] = c['tokens']\n",
    "            c['tokens'], c['entities'] = transform_text_spans(c['tokens'], c['entities'], tokenizer)\n",
    "\n",
    "        genia_data += child_data\n",
    "    \n",
    "    # Obtain dictionary of entity types\n",
    "    genia_et_freq = {}\n",
    "    for item in genia_data:\n",
    "        for e in item['entities']:\n",
    "            if e['entity_type'] not in genia_et_freq:\n",
    "                genia_et_freq[e['entity_type']] = 1\n",
    "            else:\n",
    "                genia_et_freq[e['entity_type']] += 1\n",
    "\n",
    "    genia_et = [no_entity] + list(genia_et_freq.keys())\n",
    "    genia_et_idx = {e:i for i, e in enumerate(genia_et)}\n",
    "\n",
    "    return genia_data, genia_et, genia_et_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'DNA', 'protein', 'cell_type', 'cell_line', 'RNA']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENIA_CORPUS = './data/GENIA_term_3.02/GENIAcorpus3.02.xml'\n",
    "NO_ENTITY = 'O' # Described in the IOB format\n",
    "\n",
    "genia_data, genia_et, genia_et_idx = parse_genia(GENIA_CORPUS, tokenizer, 16, NO_ENTITY)\n",
    "\n",
    "# Display entity types\n",
    "genia_et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(dataset, debug=False):\n",
    "    n_bad = 0\n",
    "    for i, item in enumerate(dataset):\n",
    "        length = len(item['tokens'])\n",
    "        for entity in item['entities']:\n",
    "            if entity['span'][1] <= length:\n",
    "                pass\n",
    "            else:\n",
    "                n_bad += 1\n",
    "                if debug:\n",
    "                    print('Wrong entity span for item %d:' % (i), item['text'])\n",
    "                    print()\n",
    "                break\n",
    "    return n_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bad = sanity_check(genia_data, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad items: 0 / 18546\n"
     ]
    }
   ],
   "source": [
    "print('Bad items: %d / %d' % (n_bad, len(genia_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split first 81%, subsequent 9%, and last 10% as train, dev and test set, respectively\n",
    "TRAIN_RATIO = 0.81\n",
    "DEV_RATIO = 0.09\n",
    "\n",
    "train_idx = [0, int(TRAIN_RATIO * len(genia_data))]\n",
    "dev_idx = [train_idx[1], train_idx[1] + int(DEV_RATIO * len(genia_data))]\n",
    "test_idx = [dev_idx[1], len(genia_data)]\n",
    "\n",
    "train_dataset = genia_data[train_idx[0]:train_idx[1]]\n",
    "dev_dataset = genia_data[dev_idx[0]:dev_idx[1]]\n",
    "test_dataset = genia_data[test_idx[0]:test_idx[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/train.genia.json'\n",
    "valid_file = './data/valid.genia.json'\n",
    "test_file = './data/test.genia.json'\n",
    "\n",
    "with open(train_file, 'w') as fp:\n",
    "    json.dump(train_dataset, fp)\n",
    "with open(valid_file, 'w') as fp:\n",
    "    json.dump(dev_dataset, fp)\n",
    "with open(test_file, 'w') as fp:\n",
    "    json.dump(test_dataset, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
